{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning \n",
    "\n",
    "### Definition:\n",
    "Changing the base model to create a unique and differentiated experience for users\n",
    "Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide as many examples in the prompt. This saves costs and enables lower-latency requests.\n",
    "\n",
    "### Benefits\n",
    "- Higher quality results than prompting\n",
    "- Ability to train on more examples than can fit in a prompt\n",
    "- Token savings due to shorter prompts\n",
    "- Lower latency requests\n",
    "\n",
    "### Steps\n",
    "1. Prepare and upload training data\n",
    "2. Train a new fine-tuned model\n",
    "3. Evaluate results and go back to step 1 if needed\n",
    "4. Use your fine-tuned model\n",
    "\n",
    "For more info check (Fine Tuning docs)[https://platform.openai.com/docs/guides/fine-tuning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Enhances Large Language Models (LLMs) by enabling them to access and use up-to-date and trustworthy information from internal knowledge bases without the need for retraining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When To Use RAG\n",
    "- **Chatbots**: A RAG chatbot can access relevant information from instruction guides, technical manuals, and other documents. Next-generation RAG 360 systems can leverage enterprise data to deliver hyper-personalized and context-aware answers.\n",
    "- **Educational software**: Using RAG Gen AI-based learning can dramatically enhance the educational experience by offering students access to answers and context-specific explanations based on topic-specific study materials.\n",
    "- **Legal tasks**: A RAG tool streamlines document reviews and legal research by drawing on the most recent legal precedents to analyze or summarize contracts, statutes, affidavits, wills, and other legal documents.\n",
    "- **Medical research**: A RAG LLM can integrate up-to-date medical data, clinical guidelines, and other information that may not have been part of the original training dataset. This helps doctors both diagnose and treat more accurately and effectively.\n",
    "- **Translation RAG**: augments language translations because it enables LLMs to grasp text context and integrate terminology and domain knowledge from internal data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Fine-Tuning  \n",
    "Personalized content recommendation For entertainment, news, and other content providers, fine-tuning a pre-trained LLM enables it to better analyze and understand each customer’s unique preferences and needs.Named-Entity Recognition (NER) Fine-tuning enables an LLM to better recognize specialized entities or terminologies (for example, legal or medical terms) where a generic LLM could fall short and generate low-quality or inaccurate responses. Sentiment analysis Fine-tuning an LLM can enhance its capabilities to better interpret the subtleties of attitude and emotion in text. This is in sharp contrast to generic LLMs that are great at understanding language but have a hard time understanding tone and intonation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG vs Fine-Tuning: How to Choose? \n",
    "- How much complexity can your team handle? \n",
    "  - Implementing RAG is less complex since it demands coding and architectural skills only. Fine-tuning requires a broader skillset that includes Natural Language Processing (NLP), deep learning, model configuration, data reprocessing, and evaluation.\n",
    "- How accurate do your responses need to be? \n",
    "  - RAG is great for generating up-to-date responses and eliminating hallucinations, but the accuracy of responses can vary in domain-specific instances. In such cases, fine-tuning is specifically designed to augment an LLM’s domain-specific understanding, resulting in more accurate responses. \n",
    "- Is your data dynamic or static? \n",
    "  - RAG is excellent for dynamic settings since it can access up-to-date data from internal knowledge sources without retraining your LLM. Fine-tuning can raise the accuracy of LLM responses, but the responses are still based on static snapshots of the training datasets and can be outdated. \n",
    "- Is budget an issue? \n",
    "  - With RAG AI, the lion’s share of expenses relates to setting up embedding and retrieval systems. The overall cost of fine-tuning is higher than that of RAG since it requires more labeled data and more computational resources running on higher-end hardware.  \n",
    "- How important is it to avoid hallucinations? \n",
    "  - RAG is less prone to hallucinations and biases because it bases each LLM response on data retrieved from an authenticated source. Fine-tuning lowers the risk of hallucinations by consuming domain-specific data but can still generate erroneous responses in the face of unfamiliar queries. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
